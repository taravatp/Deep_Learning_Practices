{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taravatp/Deep_Learning_Practices/blob/main/networks/Efficient_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "_LJ6Ta7tx6vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "eQFX1QxLvsDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Block"
      ],
      "metadata": {
        "id": "lIMMNmTbx8Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InitialBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,in_channels,out_channels,bias=False,relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if relu:\n",
        "            activation = nn.ReLU\n",
        "        else:\n",
        "            activation = nn.PReLU\n",
        "\n",
        "        self.main_branch = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels - in_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            bias=bias)\n",
        "\n",
        "        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.out_activation = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        main = self.main_branch(x)\n",
        "        ext = self.ext_branch(x)\n",
        "        out = torch.cat((main, ext), 1)\n",
        "        out = self.batch_norm(out)\n",
        "        return self.out_activation(out)"
      ],
      "metadata": {
        "id": "bfWDFMOyvsoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regular bottleneck"
      ],
      "metadata": {
        "id": "AvpqKWhex91I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularBottleneck(nn.Module):\n",
        "\n",
        "    def __init__(self,channels,internal_ratio=4,kernel_size=3,padding=0,dilation=1,asymmetric=False,dropout_prob=0,bias=False,relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if internal_ratio <= 1 or internal_ratio > channels:\n",
        "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
        "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
        "                               .format(channels, internal_ratio))\n",
        "\n",
        "        internal_channels = channels // internal_ratio\n",
        "\n",
        "        if relu:\n",
        "            activation = nn.ReLU\n",
        "        else:\n",
        "            activation = nn.PReLU\n",
        "\n",
        "        # Main branch - shortcut connection\n",
        "\n",
        "        # 1x1 projection convolution\n",
        "        self.ext_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                channels,\n",
        "                internal_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                bias=bias),\n",
        "            nn.BatchNorm2d(internal_channels),\n",
        "            activation())\n",
        "\n",
        "        # Convolution\n",
        "        if asymmetric:\n",
        "            self.ext_conv2 = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    internal_channels,\n",
        "                    internal_channels,\n",
        "                    kernel_size=(kernel_size, 1),\n",
        "                    stride=1,\n",
        "                    padding=(padding, 0),\n",
        "                    dilation=dilation,\n",
        "                    bias=bias), nn.BatchNorm2d(internal_channels), activation(),\n",
        "                nn.Conv2d(\n",
        "                    internal_channels,\n",
        "                    internal_channels,\n",
        "                    kernel_size=(1, kernel_size),\n",
        "                    stride=1,\n",
        "                    padding=(0, padding),\n",
        "                    dilation=dilation,\n",
        "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
        "        else:\n",
        "            self.ext_conv2 = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    internal_channels,\n",
        "                    internal_channels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=padding,\n",
        "                    dilation=dilation,\n",
        "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
        "\n",
        "        # 1x1 expansion convolution\n",
        "        self.ext_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                internal_channels,\n",
        "                channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                bias=bias), nn.BatchNorm2d(channels), activation())\n",
        "\n",
        "        #Regulizer\n",
        "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # PReLU layer to apply after adding the branches\n",
        "        self.out_activation = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Main branch shortcut\n",
        "        main = x\n",
        "\n",
        "        # Extension branch\n",
        "        ext = self.ext_conv1(x)\n",
        "        ext = self.ext_conv2(ext)\n",
        "        ext = self.ext_conv3(ext)\n",
        "        ext = self.ext_regul(ext)\n",
        "\n",
        "        # Add main and extension branches\n",
        "        out = main + ext\n",
        "\n",
        "        return self.out_activation(out)"
      ],
      "metadata": {
        "id": "rMIKRuDAweAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downsampling Module"
      ],
      "metadata": {
        "id": "xyynh3ATyAnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownsamplingBottleneck(nn.Module):\n",
        "    \"\"\"Downsampling bottlenecks further downsample the feature map size.\n",
        "\n",
        "    Main branch:\n",
        "    1. max pooling with stride 2; indices are saved to be used for\n",
        "    unpooling later.\n",
        "\n",
        "    Extension branch:\n",
        "    1. 2x2 convolution with stride 2 that decreases the number of channels\n",
        "    by ``internal_ratio``, also called a projection;\n",
        "    2. regular convolution (by default, 3x3);\n",
        "    3. 1x1 convolution which increases the number of channels to\n",
        "    ``out_channels``, also called an expansion;\n",
        "    4. dropout as a regularizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,in_channels,out_channels,internal_ratio=4,return_indices=False,dropout_prob=0,bias=False,relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store parameters that are needed later\n",
        "        self.return_indices = return_indices\n",
        "\n",
        "        # Check in the internal_scale parameter is within the expected range\n",
        "        # [1, channels]\n",
        "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
        "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
        "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
        "                               .format(in_channels, internal_ratio))\n",
        "\n",
        "        internal_channels = in_channels // internal_ratio\n",
        "\n",
        "        if relu:\n",
        "            activation = nn.ReLU\n",
        "        else:\n",
        "            activation = nn.PReLU\n",
        "\n",
        "        # Main branch - max pooling followed by feature map (channels) padding\n",
        "        self.main_max1 = nn.MaxPool2d(2,stride=2,return_indices=return_indices)\n",
        "\n",
        "        # 2x2 projection convolution with stride 2\n",
        "        self.ext_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                internal_channels,\n",
        "                kernel_size=2,\n",
        "                stride=2,\n",
        "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
        "\n",
        "        # Convolution\n",
        "        self.ext_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                internal_channels,\n",
        "                internal_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
        "\n",
        "        # 1x1 expansion convolution\n",
        "        self.ext_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                internal_channels,\n",
        "                out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                bias=bias), nn.BatchNorm2d(out_channels), activation())\n",
        "\n",
        "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # PReLU layer to apply after concatenating the branches\n",
        "        self.out_activation = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.return_indices:\n",
        "            main, max_indices = self.main_max1(x)\n",
        "        else:\n",
        "            main = self.main_max1(x)\n",
        "\n",
        "        # Extension branch\n",
        "        ext = self.ext_conv1(x)\n",
        "        ext = self.ext_conv2(ext)\n",
        "        ext = self.ext_conv3(ext)\n",
        "        ext = self.ext_regul(ext)\n",
        "\n",
        "        # Main branch channel padding\n",
        "        n, ch_ext, h, w = ext.size()\n",
        "        ch_main = main.size()[1]\n",
        "        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n",
        "\n",
        "        if main.is_cuda:\n",
        "            padding = padding.cuda()\n",
        "        main = torch.cat((main, padding), 1)\n",
        "        out = main + ext\n",
        "\n",
        "        return self.out_activation(out), max_indices"
      ],
      "metadata": {
        "id": "GgOMeX_MyBve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upsampling Module"
      ],
      "metadata": {
        "id": "ebL1FDMcy_yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsamplingBottleneck(nn.Module):\n",
        "    \"\"\"The upsampling bottlenecks upsample the feature map resolution using max\n",
        "    pooling indices stored from the corresponding downsampling bottleneck.\n",
        "\n",
        "    Main branch:\n",
        "    1. 1x1 convolution with stride 1 that decreases the number of channels by\n",
        "    ``internal_ratio``, also called a projection;\n",
        "    2. max unpool layer using the max pool indices from the corresponding\n",
        "    downsampling max pool layer.\n",
        "\n",
        "    Extension branch:\n",
        "    1. 1x1 convolution with stride 1 that decreases the number of channels by\n",
        "    ``internal_ratio``, also called a projection;\n",
        "    2. transposed convolution (by default, 3x3);\n",
        "    3. 1x1 convolution which increases the number of channels to\n",
        "    ``out_channels``, also called an expansion;\n",
        "    4. dropout as a regularizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,in_channels,out_channels,internal_ratio=4,dropout_prob=0,bias=False,relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Check in the internal_scale parameter is within the expected range\n",
        "        # [1, channels]\n",
        "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
        "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
        "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
        "                               .format(in_channels, internal_ratio))\n",
        "\n",
        "        internal_channels = in_channels // internal_ratio\n",
        "\n",
        "        if relu:\n",
        "            activation = nn.ReLU\n",
        "        else:\n",
        "            activation = nn.PReLU\n",
        "\n",
        "        # Main branch - max pooling followed by feature map (channels) padding\n",
        "        self.main_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "\n",
        "        # 1x1 projection convolution with stride 1\n",
        "        self.ext_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, internal_channels, kernel_size=1, bias=bias),\n",
        "            nn.BatchNorm2d(internal_channels), activation())\n",
        "\n",
        "        # Transposed convolution\n",
        "        self.ext_tconv1 = nn.ConvTranspose2d(\n",
        "            internal_channels,\n",
        "            internal_channels,\n",
        "            kernel_size=2,\n",
        "            stride=2,\n",
        "            bias=bias)\n",
        "        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)\n",
        "        self.ext_tconv1_activation = activation()\n",
        "\n",
        "        # 1x1 expansion convolution\n",
        "        self.ext_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(internal_channels, out_channels, kernel_size=1, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "        # PReLU layer to apply after concatenating the branches\n",
        "        self.out_activation = activation()\n",
        "\n",
        "    def forward(self, x, max_indices, output_size):\n",
        "        # Main branch shortcut\n",
        "        main = self.main_conv1(x)\n",
        "        main = self.main_unpool1(\n",
        "            main, max_indices, output_size=output_size)\n",
        "\n",
        "        # Extension branch\n",
        "        ext = self.ext_conv1(x)\n",
        "        ext = self.ext_tconv1(ext, output_size=output_size)\n",
        "        ext = self.ext_tconv1_bnorm(ext)\n",
        "        ext = self.ext_tconv1_activation(ext)\n",
        "        ext = self.ext_conv2(ext)\n",
        "        ext = self.ext_regul(ext)\n",
        "\n",
        "        # Add main and extension branches\n",
        "        out = main + ext\n",
        "\n",
        "        return self.out_activation(out)"
      ],
      "metadata": {
        "id": "0PX9CGAGy5ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the ENet model"
      ],
      "metadata": {
        "id": "M0DOfsal0Guy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ENet(nn.Module):\n",
        "    \"\"\"Generate the ENet model.\n",
        "\n",
        "    Keyword arguments:\n",
        "    - num_classes (int): the number of classes to segment.\n",
        "    - encoder_relu (bool, optional): When ``True`` ReLU is used as the\n",
        "    activation function in the encoder blocks/layers; otherwise, PReLU\n",
        "    is used. Default: False.\n",
        "    - decoder_relu (bool, optional): When ``True`` ReLU is used as the\n",
        "    activation function in the decoder blocks/layers; otherwise, PReLU\n",
        "    is used. Default: True.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, in_channels, encoder_relu=False, decoder_relu=True):\n",
        "        super().__init__()\n",
        "        self.initial_block = InitialBlock(in_channels, 16, relu=encoder_relu)\n",
        "\n",
        "        # Stage 1 - Encoder\n",
        "        self.downsample1_0 = DownsamplingBottleneck(16,64,return_indices=True,dropout_prob=0.01,relu=encoder_relu)\n",
        "        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
        "        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
        "        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
        "        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
        "\n",
        "        # Stage 2 - Encoder\n",
        "        self.downsample2_0 = DownsamplingBottleneck(64,128,return_indices=True,dropout_prob=0.1,relu=encoder_relu)\n",
        "        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.asymmetric2_3 = RegularBottleneck(128,kernel_size=5,padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
        "        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.asymmetric2_7 = RegularBottleneck(128,kernel_size=5,asymmetric=True,padding=2,dropout_prob=0.1,relu=encoder_relu)\n",
        "        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
        "\n",
        "        # Stage 3 - Encoder\n",
        "        self.regular3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.dilated3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.asymmetric3_2 = RegularBottleneck(128,kernel_size=5,padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
        "        self.dilated3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.regular3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.dilated3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
        "        self.asymmetric3_6 = RegularBottleneck(128,kernel_size=5,asymmetric=True,padding=2,dropout_prob=0.1,relu=encoder_relu)\n",
        "        self.dilated3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
        "\n",
        "        # Stage 4 - Decoder\n",
        "        self.upsample4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
        "        self.regular4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
        "        self.regular4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
        "\n",
        "        # Stage 5 - Decoder\n",
        "        self.upsample5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
        "        self.regular5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
        "        self.transposed_conv = nn.ConvTranspose2d(16,num_classes,kernel_size=3,stride=2,padding=1,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initial block\n",
        "        input_size = x.size()\n",
        "        x = self.initial_block(x)\n",
        "\n",
        "        # Stage 1 - Encoder\n",
        "        stage1_input_size = x.size()\n",
        "        x, max_indices1_0 = self.downsample1_0(x)\n",
        "        x = self.regular1_1(x)\n",
        "        x = self.regular1_2(x)\n",
        "        x = self.regular1_3(x)\n",
        "        x = self.regular1_4(x)\n",
        "\n",
        "        # Stage 2 - Encoder\n",
        "        stage2_input_size = x.size()\n",
        "        x, max_indices2_0 = self.downsample2_0(x)\n",
        "        x = self.regular2_1(x)\n",
        "        x = self.dilated2_2(x)\n",
        "        x = self.asymmetric2_3(x)\n",
        "        x = self.dilated2_4(x)\n",
        "        x = self.regular2_5(x)\n",
        "        x = self.dilated2_6(x)\n",
        "        x = self.asymmetric2_7(x)\n",
        "        x = self.dilated2_8(x)\n",
        "\n",
        "        # Stage 3 - Encoder\n",
        "        x = self.regular3_0(x)\n",
        "        x = self.dilated3_1(x)\n",
        "        x = self.asymmetric3_2(x)\n",
        "        x = self.dilated3_3(x)\n",
        "        x = self.regular3_4(x)\n",
        "        x = self.dilated3_5(x)\n",
        "        x = self.asymmetric3_6(x)\n",
        "        x = self.dilated3_7(x)\n",
        "\n",
        "        # Stage 4 - Decoder\n",
        "        x = self.upsample4_0(x, max_indices2_0, output_size=stage2_input_size)\n",
        "        x = self.regular4_1(x)\n",
        "        x = self.regular4_2(x)\n",
        "\n",
        "        # Stage 5 - Decoder\n",
        "        x = self.upsample5_0(x, max_indices1_0, output_size=stage1_input_size)\n",
        "        x = self.regular5_1(x)\n",
        "        x = self.transposed_conv(x, output_size=input_size)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ur9aoA7j0EPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing ENet model"
      ],
      "metadata": {
        "id": "1L5TKQEk1zOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  model = ENet(num_classes=3, in_channels=12)\n",
        "  input = torch.rand((8,12,64,64))\n",
        "  output = model(input)\n",
        "  print('output:',output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNYcoEc611Nk",
        "outputId": "6b5b5e0a-415d-4ccd-e7e2-3bf25bc34f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: torch.Size([8, 3, 64, 64])\n"
          ]
        }
      ]
    }
  ]
}