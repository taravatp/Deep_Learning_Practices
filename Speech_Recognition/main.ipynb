{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zwm0WPWrlUA"
      },
      "source": [
        "# Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhaqURLJfyBX",
        "outputId": "c200c52f-c9a2-49c5-b299-b22961a0c45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUuovSNND6Mv"
      },
      "outputs": [],
      "source": [
        "!pip install -qU hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMq4ArVOrVZ4",
        "outputId": "0bf0faad-15d9-40b3-e9a5-8d0d2126fec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f94989ccdf0>, 'Connection to us-python.pkg.dev timed out. (connect timeout=15)')': /colab-wheels/public/simple/torchmetrics/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SQEuHY4gIK5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "\n",
        "import torchmetrics\n",
        "from torchmetrics import BLEUScore\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import hazm\n",
        "from hazm import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocfL9cvqnIjK",
        "outputId": "02ca6a3a-28b5-4a4c-d54c-a4d94d3063b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n",
            "2.0.2+cu118\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV1P04g4nEzh",
        "outputId": "d1b33eb6-d753-4fd7-d7df-73e47ccbdbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq9d5XAomTvx",
        "outputId": "a20e20e3-aa9c-4353-8b3a-de6c8974abba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DL_hw4\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/DL_hw4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7hZ3rUfnM9w"
      },
      "outputs": [],
      "source": [
        "# !pip install unrar\n",
        "# !unrar x '/content/drive/MyDrive/DL_hw4/DL-HW4-Dataset.rar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAK_G-k0EmPW"
      },
      "source": [
        "# Creating the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evt-e8jLE_jn"
      },
      "outputs": [],
      "source": [
        "class dictionary:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.index2word = {0: \"SOS\", 1:\"EOS\"}\n",
        "    self.word2count = {\"SOS\":0 , \"EOS\":1}\n",
        "    self.n_words = 2\n",
        "    self.normalizer = hazm.Normalizer()\n",
        "\n",
        "  def clean(self,text):\n",
        "    text = text.strip()\n",
        "    text = self.normalizer.normalize(text) #normalizing\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text) # inserting a space between words and punctuations\n",
        "    text = re.sub(\"\\s+\", \" \", text) #removing redundant white spaces\n",
        "    return text\n",
        "\n",
        "  def add_setence_to_lang(self,sentence):\n",
        "    for token in word_tokenize(sentence):\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.n_words\n",
        "        self.word2count[token] = 1\n",
        "        self.index2word[self.n_words] = token\n",
        "        self.n_words +=1\n",
        "      else:\n",
        "        self.word2count[token] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZiUiQsMEovX"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL_hw4/DL-Hw4-Dataset/Persian-Speech-To-Text-Maps.xlsx'\n",
        "dataset = pd.read_excel(DATASET_PATH)\n",
        "\n",
        "Dictionary = dictionary()\n",
        "for index, row in dataset.iterrows():\n",
        "  text = row['text']\n",
        "  text = Dictionary.clean(text)\n",
        "  Dictionary.add_setence_to_lang(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjSa5JLkXAgs",
        "outputId": "82a05318-97f2-4c22-94b4-51b8779051b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in the dictionary: 2367\n"
          ]
        }
      ],
      "source": [
        "print('Number of words in the dictionary:',Dictionary.n_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ui1GaS9rh-b"
      },
      "source": [
        "# Creating the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0mx8f3sS20z"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG1HWZalkuoM"
      },
      "outputs": [],
      "source": [
        "class dataset_SR(Dataset):\n",
        "\n",
        "  def __init__(self,Dictionary,flag):\n",
        "\n",
        "    self.audios_root = '/content/drive/MyDrive/DL_hw4/DL-Hw4-Dataset/Persian-Speech-To-Text-Audios'\n",
        "    dataset_path = '/content/drive/MyDrive/DL_hw4/DL-Hw4-Dataset/Persian-Speech-To-Text-Maps.xlsx'\n",
        "    self.dataset = pd.read_excel(dataset_path)\n",
        "    self.Dictionary = Dictionary\n",
        "    self.normalizer = hazm.Normalizer()\n",
        "    train_data, test_data = train_test_split(self.dataset, test_size=0.1, random_state=42)\n",
        "    train_data, validation_data =  train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "    if flag == 'train':\n",
        "      self.dataset = train_data\n",
        "      self.dataset = self.dataset.reset_index(drop=True)\n",
        "    elif flag == 'validation':\n",
        "      self.dataset = validation_data\n",
        "      self.dataset = self.dataset.reset_index(drop=True)\n",
        "    elif flag == 'test':\n",
        "      self.dataset = test_data\n",
        "      self.dataset = self.dataset.reset_index(drop=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def preprocess_audio(self,audio_path):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    spectrogram_transform = transforms.Spectrogram(n_fft=128, hop_length=4096)\n",
        "    spectrogram = spectrogram_transform(waveform)\n",
        "    return spectrogram\n",
        "\n",
        "  def preprocess_text(self,text):\n",
        "    text = text.strip()\n",
        "    text = self.normalizer.normalize(text) #normalizing\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text) # inserting a space between words and punctuations\n",
        "    text = re.sub(\"\\s+\", \" \", text) #removing redundant white spaces\n",
        "\n",
        "    vector = [SOS_token]\n",
        "    for word in word_tokenize(text):\n",
        "      vector.append(self.Dictionary.word2index[word])\n",
        "    vector.append(EOS_token)\n",
        "    vector = torch.tensor(vector, dtype=torch.long)\n",
        "    return vector\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    audio_name = self.dataset.loc[index,'audio']\n",
        "    audio_name = audio_name.split('/')[-1]\n",
        "    audio_path = os.path.join(self.audios_root,audio_name)\n",
        "    audio = self.preprocess_audio(audio_path)\n",
        "\n",
        "    text = self.dataset.loc[index,'text']\n",
        "    text = self.preprocess_text(text)\n",
        "    return audio,text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgrt1dXAviRP"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6W2aTaruj9x"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,inp_dim,hid_dim,n_layers,dropout,layer_type):\n",
        "    super().__init__()\n",
        "    self.layer_type = layer_type\n",
        "    if layer_type == \"RNN\":\n",
        "      self.rnn = nn.RNN(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    elif layer_type == \"GRU\":\n",
        "      self.rnn = nn.GRU(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    elif layer_type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,hidden,cell=None):\n",
        "\n",
        "    if self.layer_type == \"RNN\":\n",
        "      output, hidden = self.rnn(x, hidden)\n",
        "      return hidden\n",
        "\n",
        "    elif self.layer_type == \"GRU\":\n",
        "      output, hidden = self.rnn(x, hidden)\n",
        "      return hidden\n",
        "\n",
        "    elif self.layer_type == \"LSTM\":\n",
        "      outputs, (hidden,cell) = self.rnn(x,(hidden,cell))\n",
        "      return hidden,cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmJKQbzT28xf"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,inp_dim,hid_dim,output_dim,n_layers,dropout,layer_type):\n",
        "\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(output_dim,inp_dim)\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "    if layer_type == \"RNN\":\n",
        "      self.rnn = nn.RNN(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    elif layer_type == \"GRU\":\n",
        "      self.rnn = nn.GRU(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "    elif layer_type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(inp_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.activation_layer = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self,input,hidden,cell=None):\n",
        "    #input = [1]\n",
        "    input = input.unsqueeze(0) #input = [1,1]\n",
        "    x = self.embedding(input) #embedding = [1,1,128]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    if self.layer_type == \"RNN\":\n",
        "      output, hidden = self.rnn(x, hidden)\n",
        "      output = self.fc_out(output.squeeze(0))\n",
        "      prediction = self.activation_layer(output)\n",
        "      return prediction, hidden\n",
        "\n",
        "    elif self.layer_type == \"GRU\":\n",
        "      output, hidden = self.rnn(x, hidden)\n",
        "      output = self.fc_out(output.squeeze(0))\n",
        "      prediction = self.activation_layer(output)\n",
        "      return prediction, hidden\n",
        "\n",
        "    elif self.layer_type == \"LSTM\":\n",
        "      output, (hidden,cell) = self.rnn(x,(hidden,cell))\n",
        "      output = self.fc_out(output.squeeze(0))\n",
        "      prediction = self.activation_layer(output)\n",
        "      return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW9vUrY0Bizk"
      },
      "source": [
        "# Instantiating objects and setting hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oRnp1r7XzV-"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = 65\n",
        "HIDDEN_SIZE = 128\n",
        "OUTPUT_SIZE = Dictionary.n_words\n",
        "\n",
        "NUM_LAYERS = 2\n",
        "MAX_LEN =  20\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 1\n",
        "TEACHER_FORCE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6uoV3LNBlh1"
      },
      "outputs": [],
      "source": [
        "train_data = dataset_SR(Dictionary, flag = 'train')\n",
        "validation_data = dataset_SR(Dictionary, flag = 'validation')\n",
        "test_data = dataset_SR(Dictionary, flag = 'test')\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwNYQR88yJvd"
      },
      "outputs": [],
      "source": [
        "def build_modules(layer_type):\n",
        "  encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, dropout=0.2, layer_type = layer_type).to(device)\n",
        "  decoder = Decoder(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, dropout=0.2,  layer_type = layer_type).to(device)\n",
        "\n",
        "  encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=LEARNING_RATE)\n",
        "  decoder_optimizer = torch.optim.Adam(decoder.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "  return encoder, decoder, encoder_optimizer, decoder_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXSyAOv5BmRs"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMAcb_WyBnym"
      },
      "outputs": [],
      "source": [
        "def train(encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
        "\n",
        "  loss = 0\n",
        "  for iter,batch in enumerate(train_dataloader):\n",
        "    loss = 0\n",
        "    audio = batch[0].to(device) #[batch_size, 1, num_bins, num_frames]\n",
        "    text  = batch[1].to(device)  #[batch_size, num_tokens]\n",
        "\n",
        "    num_frames = audio.shape[-1] #num_frames\n",
        "    num_word_tokens = text.shape[-1] #num_tokens\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_hidden = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "    encoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "    encoder_outputs = torch.zeros(num_frames,NUM_LAYERS,BATCH_SIZE, HIDDEN_SIZE).to(device) #[num_frames, NUM_LAYERS, BATCH_SIZE, 64]\n",
        "\n",
        "    for index in range(num_frames):\n",
        "      audio_frame = audio[:,:,:,index] #[1,1,128]\n",
        "\n",
        "      if encoder.layer_type == \"LSTM\":\n",
        "        encoder_hidden, encoder_cell = encoder(audio_frame,encoder_hidden,encoder_cell) # encoder_output: [1,1,64] - encdoder_hidden: [num_layers,batch_size,64]\n",
        "      else: # GRU or simple RNN\n",
        "        encoder_hidden = encoder(audio_frame,encoder_hidden)\n",
        "\n",
        "      encoder_outputs[index,:,:,:] = encoder_hidden\n",
        "\n",
        "    # decoder_input = torch.tensor([[SOS_token]]).to(device)\n",
        "    decoder_hidden = encoder_hidden #hidden state of decoder is equal to the last hidden state of encoder\n",
        "    decoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "\n",
        "    for index in range(num_word_tokens-1):\n",
        "      decoder_input = text[:,index]\n",
        "      if decoder.layer_type == \"LSTM\":\n",
        "        decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "      else:\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "      loss += criterion(decoder_output, text[:,index+1])\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "  torch.save(encoder.state_dict(), f\"encoder{encoder.layer_type}.pth\")\n",
        "  torch.save(decoder.state_dict(), f\"decoder{decoder.layer_type}.pth\")\n",
        "  return loss , encoder, decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRC51p4yvl7C"
      },
      "outputs": [],
      "source": [
        "def validate(encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
        "\n",
        "  loss = 0\n",
        "  for iter,batch in enumerate(validation_dataloader):\n",
        "    loss = 0\n",
        "    audio = batch[0].to(device) #[batch_size, 1, num_bins, num_frames]\n",
        "    text  = batch[1].to(device)  #[batch_size, num_tokens]\n",
        "\n",
        "    num_frames = audio.shape[-1] #num_frames\n",
        "    num_word_tokens = text.shape[-1] #num_tokens\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_hidden = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "    encoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "    encoder_outputs = torch.zeros(num_frames,NUM_LAYERS,BATCH_SIZE, HIDDEN_SIZE).to(device) #[num_frames, NUM_LAYERS, BATCH_SIZE, 64]\n",
        "\n",
        "    for index in range(num_frames):\n",
        "      audio_frame = audio[:,:,:,index] #[1,1,128]\n",
        "\n",
        "      if encoder.layer_type == \"LSTM\":\n",
        "        encoder_hidden, encoder_cell = encoder(audio_frame,encoder_hidden,encoder_cell) # encoder_output: [1,1,64] - encdoder_hidden: [1,1,64]\n",
        "      else: # GRU or simple RNN\n",
        "        encoder_hidden = encoder(audio_frame,encoder_hidden)\n",
        "\n",
        "      encoder_outputs[index,:,:,:] = encoder_hidden\n",
        "\n",
        "    # decoder_input = torch.tensor([[SOS_token]]).to(device)\n",
        "    decoder_hidden = encoder_hidden #hidden state of decoder is equal to the last hidden state of encoder\n",
        "    decoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "\n",
        "    for index in range(num_word_tokens-1):\n",
        "      decoder_input = text[:,index]\n",
        "\n",
        "      if decoder.layer_type == \"LSTM\":\n",
        "        decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "      else:\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "      loss += criterion(decoder_output, text[:,index+1])\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFm9OdH2yCwG"
      },
      "source": [
        "# Training and evaluating different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJDfJy-QwAj1"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_losses = []\n",
        "  validation_losses = []\n",
        "  encoder, decoder, encoder_optimizer, decoder_optimizer = build_modules(\"RNN\")\n",
        "\n",
        "  for i in range(NUM_EPOCHS):\n",
        "    print('##########')\n",
        "    train_loss, encoder, decoder = train(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "    validation_loss = validate(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "    print(f'train loss after epoch {i} is: {train_loss/len(train_dataloader)}')\n",
        "    print(f'validation loss after epoch {i} is: {validation_loss/len(validation_dataloader)}')\n",
        "\n",
        "    train_losses.append(train_loss/len(train_dataloader))\n",
        "    validation_losses.append(validation_loss/len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE5tHh7JXBFc"
      },
      "outputs": [],
      "source": [
        "train_losses_per_epoch = [loss.item() for loss in train_losses]\n",
        "validation_losses_per_epoch = [loss.item() for loss in validation_losses]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.plot(train_losses_per_epoch, \"ro-\", label=\"Train\")\n",
        "plt.plot(validation_losses_per_epoch, \"go-\", label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H430zropw5zN"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_losses = []\n",
        "  validation_losses = []\n",
        "  encoder, decoder, encoder_optimizer, decoder_optimizer = build_modules(\"GRU\")\n",
        "\n",
        "  for i in range(NUM_EPOCHS):\n",
        "    print('##########')\n",
        "    train_loss, encoder, decoder = train(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "    validation_loss = validate(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "    print(f'train loss after epoch {i} is: {train_loss/len(train_dataloader)}')\n",
        "    print(f'validation loss after epoch {i} is: {validation_loss/len(validation_dataloader)}')\n",
        "\n",
        "    train_losses.append(train_loss/len(train_dataloader))\n",
        "    validation_losses.append(validation_loss/len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_per_epoch = [loss.item() for loss in train_losses]\n",
        "validation_losses_per_epoch = [loss.item() for loss in validation_losses]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.plot(train_losses_per_epoch, \"ro-\", label=\"Train\")\n",
        "plt.plot(validation_losses_per_epoch, \"go-\", label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")"
      ],
      "metadata": {
        "id": "LG423Jx6m6Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc_zLenJzPDI"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_losses = []\n",
        "  validation_losses = []\n",
        "  encoder, decoder, encoder_optimizer, decoder_optimizer = build_modules(\"LSTM\")\n",
        "\n",
        "  for i in range(NUM_EPOCHS):\n",
        "    print('##########')\n",
        "    train_loss, encoder, decoder = train(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "    validation_loss = validate(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "    print(f'train loss after epoch {i} is: {train_loss/len(train_dataloader)}')\n",
        "    print(f'validation loss after epoch {i} is: {validation_loss/len(validation_dataloader)}')\n",
        "\n",
        "    train_losses.append(train_loss/len(train_dataloader))\n",
        "    validation_losses.append(validation_loss/len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFdgefqeFKq_"
      },
      "outputs": [],
      "source": [
        "train_losses_per_epoch = [loss.item() for loss in train_losses]\n",
        "validation_losses_per_epoch = [loss.item() for loss in validation_losses]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.plot(train_losses_per_epoch, \"ro-\", label=\"Train\")\n",
        "plt.plot(validation_losses_per_epoch, \"go-\", label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder,decoder):\n",
        "  bleu = BLEUScore()\n",
        "  MAX_LEN = 20\n",
        "  bluescore = 0\n",
        "  with torch.no_grad():\n",
        "    for iter,batch in enumerate(test_dataloader):\n",
        "\n",
        "      audio = batch[0].to(device) #[batch_size, 1, num_bins, num_frames]\n",
        "      text  = batch[1] #[batch_size, num_tokens]\n",
        "\n",
        "      num_frames = audio.shape[-1] #num_frames\n",
        "\n",
        "      encoder_hidden = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "      encoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "      encoder_outputs = torch.zeros(num_frames,NUM_LAYERS,BATCH_SIZE, HIDDEN_SIZE).to(device) #[num_frames, NUM_LAYERS, BATCH_SIZE, 64]\n",
        "\n",
        "      for index in range(num_frames):\n",
        "        audio_frame = audio[:,:,:,index] #[1,1,128]\n",
        "        if encoder.layer_type == \"LSTM\":\n",
        "          encoder_hidden, encoder_cell = encoder(audio_frame,encoder_hidden,encoder_cell) # encoder_output: [1,1,64] - encdoder_hidden: [1,1,64]\n",
        "        else: # GRU or simple RNN\n",
        "          encoder_hidden = encoder(audio_frame,encoder_hidden)\n",
        "        encoder_outputs[index,:,:,:] = encoder_hidden\n",
        "\n",
        "      decoder_hidden = encoder_hidden\n",
        "      decoder_cell = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE).to(device) #[1, 1, 64]\n",
        "\n",
        "      decoded_words = []\n",
        "      decoder_input = torch.tensor([SOS_token]).to(device)\n",
        "\n",
        "      for index in range(MAX_LEN):\n",
        "\n",
        "        if decoder.layer_type == \"LSTM\":\n",
        "\n",
        "          decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
        "        else:\n",
        "          decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "        topv, topi = decoder_output.data.topk(1)\n",
        "        if topi.item() == EOS_token:\n",
        "          decoded_words.append('<EOS>')\n",
        "          break\n",
        "        else:\n",
        "          decoded_words.append(Dictionary.index2word[topi.item()])\n",
        "          decoder_input = torch.tensor([topi.item()]).to(device)\n",
        "\n",
        "      input_text = []\n",
        "      text = torch.squeeze(text)\n",
        "\n",
        "      for index in text:\n",
        "        ind = int(index.item())\n",
        "        word = Dictionary.index2word[ind]\n",
        "        input_text.append(word)\n",
        "\n",
        "      print(decoded_words)\n",
        "      print(input_text)\n",
        "      bluescore +=  bleu(decoded_words,input_text)\n",
        "\n",
        "\n",
        "  return bluescore"
      ],
      "metadata": {
        "id": "e5Ckc9xwpnzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate(encoder,decoder))"
      ],
      "metadata": {
        "id": "a962jQl6ppva"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}